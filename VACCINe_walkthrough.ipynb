{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VACCINe - Walkthrough\n",
    "In this tutorial we will go over steps neseccarry to access, manipulate, and join disparate Social Determinants of Health datasets to create a final product.\n",
    "\n",
    "## Predefined functions\n",
    "First, we need to create a series of functions which will we use throughout this walkthrough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### geo_to_fips\n",
    "`geo_to_fips(dataframe, latitude, longitude)`\n",
    "\n",
    "We use this function to convert pairs of latitudes and longitudes to 15-digits FIPS codes. \n",
    "\n",
    "#### Input\n",
    "* First argument (`dataframe`) is a dataframe containing at least a latitude and longitude columns\n",
    "* Second argument (`latitude`) must be the name of the column which stores the latitude addresses (as a string)\n",
    "* Third argument (`longitude`) must be the name of the column which stores the longitude addresses (as a string)\n",
    "* **NOTE:** For larger datasets (more than 10000 rows), please be sure to break up the dataset into smaller subsets before using this function. This function uses the Federal Communications Commission's API to convert latitude and longitude pairs to FIPS codes.\n",
    "\n",
    "#### Output\n",
    "* The output is a new pandas dataframe with an additional column containing the 15-digits FIPS codes\n",
    "    * NOTE: you should save the result of the function in a new variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_to_fips(dataframe, latitude, longitude):\n",
    "    fips = []\n",
    "    for index, value in enumerate(dataframe.iloc[:,0]):\n",
    "        url = \"https://geo.fcc.gov/api/census/block/find?latitude=\" + str(dataframe[latitude][index]) + \"&longitude=\"+ str(dataframe[longitude][index]) + \"&format=json\"\n",
    "        obj = json.load(urlopen(url))\n",
    "        fips.append(obj['Block']['FIPS'])\n",
    "    dataframe[\"FIPS\"] = fips\n",
    "    print(\"FIPS code converted\")\n",
    "    return  dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fips_check\n",
    "`fips_check(dataframe, col)`\n",
    "\n",
    "We use this function to identify the length of FIPS code in any dataset. This function displays the length of the largest FIPS code in a column.\n",
    "\n",
    "#### Input\n",
    "* first argument (`dataframe`) is the name of your dataset\n",
    "* second argument (`col`) is the name of the column which holds the FIPS codes\n",
    "    * NOTE: col must be a string variable\n",
    "\n",
    "#### Output\n",
    "* Returns the length of the largest FIPS code in `col` column of your dataset `dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fips_check(dataframe, col):\n",
    "    tract = []\n",
    "    for n in dataframe[col]:\n",
    "        tract.append(len(str(n)))\n",
    "    return max(tract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fips_11 & fips_12\n",
    "`fips_11(dataframe, column_list = [])` & `fips_12(dataframe, column_list = [])`\n",
    "\n",
    "We will use these two functions frequently throughout this whole walkthrough to convert the 15-digits FIPS codes to 11 or 12-digits FIPS codes. These functions also serve as slicer, to reduce the main datasets to only the neseccary columns.\n",
    "\n",
    "#### Input: \n",
    "* First argument (`dataframe`) is the variable containing the original dataset \n",
    "* Second argument (`column_list = []`) is the LIST variable where you can name all the columns you would like to include in the final dataset. You can list the columns in any order you would like, as long as the first item is the column with the FIPS code.\n",
    "    * NOTE: column_list is a list variable containing all column names as strings\n",
    "    * NOTE: the first column in your original dataset should be the column containing the geogrphical unit\n",
    "\n",
    "#### Output:\n",
    "* The output is a pandas dataframe with the desired columns and 11 (or 12) digits FIPS codes.\n",
    "* The output also includes a column with the geographical unit of interest renmaed to _FIPS_.\n",
    "* The output should be saved in a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fips_11(dataframe, column_list = []):\n",
    "    df_new = dataframe.loc[:, column_list]\n",
    "    df_new[column_list[0]] = df_new[column_list[0]].astype(str)\n",
    "    df_new['CT'] = df_new[column_list[0]].str[0:11].astype(str)\n",
    "    return df_new\n",
    "\n",
    "\n",
    "\n",
    "def fips_12(dataframe, column_list = []):\n",
    "    df_new = dataframe.loc[:, column_list]\n",
    "    df_new[column_list[0]] = df_new[column_list[0]].astype(str)\n",
    "    df_new['BG'] = df_new[column_list[0]].str[0:12].astype(str)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join_to_vaccine\n",
    "`join_to_vaccine(V, X)`\n",
    "\n",
    "#### Input\n",
    "\n",
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_to_vaccine(V, V_index, X, X_index):\n",
    "    V[V_index] = V[V_index].astype(str)\n",
    "    X[X_index] = X[X_index].astype(str)\n",
    "    return V.join(X.set_index(X_index), on=V_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules\n",
    "Modules below are all essential and must be installed in your system. Please use the command below to install any module. \n",
    "\n",
    "### In command line\n",
    "`pip install pandas` You can replace pandas with the module of interest\n",
    "\n",
    "### In Jupyter\n",
    "To install any module directly in Jupyter, you can add `!` at the beginning of your command. See below:\n",
    "`! pip install pandas` You can replace pandas with the module of interest\n",
    "\n",
    "\n",
    "\n",
    "from census import Census\n",
    "from us import states\n",
    "import pandas as pd \n",
    "import json\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import zipfile\n",
    "import requests, zipfile, io\n",
    "from datetime import date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from census import Census\n",
    "from us import states\n",
    "import pandas as pd \n",
    "import json\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import zipfile\n",
    "import requests, zipfile, io\n",
    "from datetime import date "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## American Community Survey (ACS)\n",
    "We will use ACS's API to access its data.\n",
    "\n",
    "One important point to mention is we chose to start constructing VACCINe by incorporating ACS data, since ACS has data for **ALL** of FIPS codes. Another way to go about constructing your main Social Determinants of Health dataset is to prepopulate all FIPS codes in a column of an empty dataset. Then you would be able to join additional datasets to this column. You can easily use ACS's API to generate the FIPS column.\n",
    "\n",
    "### API setup\n",
    "In order to use the API, you must acquire an API key. See address below:\n",
    "https://api.census.gov/data/key_signup.html\n",
    "\n",
    "To read the detailed API documentation, please visit the website below:\n",
    "https://github.com/datamade/census "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Census(\"f42ca37e5880674136047af378c64f8e12de51f4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing the variables of interest\n",
    "Below we create a dictionary from the variables we want to include in the dataset and a human readable name for each. Note that you can name these variables based on your preference. For example we know `B27010_023E` corresponds to number of 18 to 34 years olds who only have Medicaid. We chose to name this variable `18_34y__MedcaidOnly`, however, you have the flexibility to name this based on your liking. \n",
    "\n",
    "This step will be crucial to \"decode\" ambigious variable names in our dataset and rename the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataHeadDict = {\n",
    "    \"NAME\" : \"Description\",\n",
    "    \"B99051_001E\":\"fbTotal\",\n",
    "    \"B99051_005E\" : \"foreignBorn\",\n",
    "    \n",
    "    \"B27010_033E\" : \"18_34y__NOinsurance\", \n",
    "    \"B27010_050E\" : \"35_64y__NOinsurance\",\n",
    "    \"B27010_018E\" : \"18_34y__total\",\n",
    "    \"B27010_034E\": \"35_64y__total\", \n",
    "    \"B27010_023E\" : \"18_34y__MedcaidOnly\", \n",
    "    \"B27010_039E\" : \"35_64y__MedcaidOnly\",\n",
    "    \"B27010_029E\" : \"18_34y__Medicare/Medicaid\",\n",
    "    \"B27010_046E\" : \"35_64y__Medicare/Medicaid\", \n",
    "    \n",
    "    \"B15003_001E\": \"totalPop_b\",\n",
    "    \"B15003_017E\" : \"high_school_diploma\", \n",
    "    \"B15003_018E\" : \"GED\", \n",
    "    \"B15003_002E\" : \"no_school\", \n",
    "    \"B15003_003E\" : \"nursery_school\", \n",
    "    \"B15003_004E\" : \"kindergarten\", \n",
    "    \"B15003_005E\" : \"grade1_school\", \n",
    "    \"B15003_006E\" : \"grade2_school\", \n",
    "    \"B15003_007E\" : \"grade3_school\",\n",
    "    \"B15003_008E\" : \"grade4_school\",\n",
    "    \"B15003_009E\" : \"grade5_school\",\n",
    "    \"B15003_010E\" : \"grade6_school\",\n",
    "    \"B15003_011E\" : \"grade7_school\",\n",
    "    \"B15003_012E\" : \"grade8_school\",\n",
    "    \"B15003_013E\" : \"grade9_school\",\n",
    "    \"B15003_014E\" : \"grade10_school\",\n",
    "    \"B15003_015E\" : \"grade11_school\",\n",
    "    \"B15003_016E\" : \"grade12_school_no_diploma\", \n",
    "    \n",
    "    \"B23025_005E\" : \"unemployed_labor_force_16up\", \n",
    "    \"B23025_002E\" : \"labor_force_16up\", \n",
    "    \n",
    "    \"B19113_001E\" : \"medianHouseIncome\", \n",
    "    \n",
    "    \"C17002_001E\" : \"totalPop_a\", \n",
    "    \"C17002_002E\" : \"estimateUnder_.50\",\n",
    "    \"C17002_003E\" : \"estimate_.50_.99\",\n",
    "    \n",
    "    \"B19083_001E\" : \"GINI\",\n",
    "    \"B01003_001E\" : \"totalPop\"\n",
    "}\n",
    "\n",
    "censusCodes = list(dataHeadDict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables taken from ACS 5 year estimate 2016 as listed above. `censusCodes` variable contains only the keys in `dataHeadDict` (i.e. the coded variable names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACS 5 year estimate 2016.\n",
    "acs5DataCT = c.acs5.get(censusCodes, geo = {'for': 'tract:*', 'in':'state:{}'.format(states.NY.fips)}, year =2016)\n",
    "dfCT = pd.DataFrame(data = acs5DataCT)\n",
    "\n",
    "#create census tract fips code \n",
    "dfCT['Census_Tract'] = dfCT['state'].map(str) + dfCT['county'].map(str) + dfCT['tract'].map(str)\n",
    "\n",
    "# include only NYC in dfCT\n",
    "dfCT = dfCT[(dfCT.county == '005')| (dfCT.county =='047')| (dfCT.county == '061') |(dfCT.county == '081')| (dfCT.county == '085')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are not only limited to variables you can pull from ACS dataset. You can combine them when appropriate to derive new variables. Below we are calculate the percentage for some of these variables based on the total population of each geographical area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCT['foriegnBornPercent'] = dfCT.B99051_005E / dfCT.B99051_001E *100\n",
    "dfCT['woInsurance18_64Percent'] = (dfCT.B27010_033E + dfCT.B27010_050E) / (dfCT.B27010_018E + dfCT.B27010_034E)  *100\n",
    "dfCT['medicaid18_64Percent'] = (dfCT.B27010_023E + dfCT.B27010_039E) / (dfCT.B27010_018E + dfCT.B27010_034E)  *100\n",
    "dfCT['medicaid_Medicare18_64Percent'] = (dfCT.B27010_029E + dfCT.B27010_046E) / (dfCT.B27010_018E + dfCT.B27010_034E)  *100\n",
    "dfCT['no_school_to_12_grade_no_diplomaPercent'] = (dfCT.B15003_002E + dfCT.B15003_003E + dfCT.B15003_004E +  dfCT.B15003_005E+ \\\n",
    "                                                dfCT.B15003_006E+  dfCT.B15003_007E+  dfCT.B15003_008E+  dfCT.B15003_009E+  \\\n",
    "                                               dfCT.B15003_010E+  dfCT.B15003_011E+  dfCT.B15003_012E+  dfCT.B15003_013E+  \\\n",
    "                                               dfCT.B15003_014E+  dfCT.B15003_015E+  dfCT.B15003_016E) / dfCT.B15003_001E  *100\n",
    "dfCT['highSchool_GEDPercent'] = (dfCT.B15003_017E + dfCT.B15003_018E) / dfCT.B15003_001E  *100\n",
    "dfCT['unemployedPercent'] = dfCT.B23025_005E / dfCT.B23025_002E *100\n",
    "\n",
    "dfCT['medianHouseIncome'] = dfCT.B19113_001E \n",
    "dfCT['belowPovertyPercent'] = (dfCT.C17002_002E + dfCT.C17002_003E) / dfCT.C17002_001E *100 \n",
    "dfCT['GINI'] = dfCT['B19083_001E'].apply(pd.to_numeric)\n",
    "dfCT['totalPopulation'] = dfCT.B01003_001E\n",
    "\n",
    "#remove original raw data \n",
    "dfCT = dfCT.drop(censusCodes, axis =1)\n",
    "dfCT = dfCT.drop(['county','state','tract'], axis =1)\n",
    "dfCT = dfCT.reset_index(drop = True)\n",
    "dfCT.rename(columns={dfCT.columns[0]:'FIPS'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air quality - Environmental Protection Agency\n",
    "This data set has several useful variables. However, we are only using _Total Respiratory (hazard quotient)_.\n",
    "\n",
    "### Reading the dataset\n",
    "We access the dataset directly from EPA's website, by reading the URL in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_epa = \"https://www.epa.gov/sites/production/files/2018-08/nata2014v2_national_resphi_by_tract_poll.xlsx\"\n",
    "epa_df = pd.read_excel(url_epa, sheet_name=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset\n",
    "It's a good practice to explore the dataset at hands by looking at the column names (or data dictionary if available), some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['State', 'EPA Region', 'County', 'FIPS', 'Tract', 'Population',\n",
       "       'Total Respiratory (hazard quotient)', '1,2-EPOXYBUTANE',\n",
       "       '1,3-DICHLOROPROPENE', '2-CHLOROACETOPHENONE',\n",
       "       '2,4-TOLUENE DIISOCYANATE', '4,4'-METHYLENEDIPHENYL DIISOCYANATE (MDI)',\n",
       "       'ACETALDEHYDE', 'ACROLEIN', 'ACRYLIC ACID', 'ACRYLONITRILE',\n",
       "       'ANTIMONY COMPOUNDS', 'BERYLLIUM COMPOUNDS',\n",
       "       'BIS(2-ETHYLHEXYL)PHTHALATE (DEHP)', 'CHROMIUM VI (HEXAVALENT)',\n",
       "       'CHLORINE', 'CHLOROPRENE', 'COBALT COMPOUNDS', 'DIESEL PM',\n",
       "       'DIETHANOLAMINE', 'EPICHLOROHYDRIN',\n",
       "       'ETHYLENE DIBROMIDE (DIBROMOETHANE)', 'ETHYLENE GLYCOL', 'FORMALDEHYDE',\n",
       "       'HEXACHLOROCYCLOPENTADIENE', 'HEXAMETHYLENE DIISOCYANATE',\n",
       "       'HYDROCHLORIC ACID (HYDROGEN CHLORIDE [GAS ONLY])', 'MALEIC ANHYDRIDE',\n",
       "       'METHYL BROMIDE (BROMOMETHANE)', 'METHYL ISOCYANATE',\n",
       "       'METHYL METHACRYLATE', 'METHYLENE CHLORIDE', 'NICKEL COMPOUNDS',\n",
       "       'NAPHTHALENE', 'NITROBENZENE', 'PHOSGENE', 'PHTHALIC ANHYDRIDE',\n",
       "       'PROPIONALDEHYDE', 'PROPYLENE DICHLORIDE (1,2-DICHLOROPROPANE)',\n",
       "       'PROPYLENE OXIDE', 'STYRENE OXIDE', 'TITANIUM TETRACHLORIDE',\n",
       "       'TRIETHYLAMINE', 'VINYL ACETATE', '1,4-DIOXANE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epa_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>EPA Region</th>\n",
       "      <th>County</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Tract</th>\n",
       "      <th>Population</th>\n",
       "      <th>Total Respiratory (hazard quotient)</th>\n",
       "      <th>1,2-EPOXYBUTANE</th>\n",
       "      <th>1,3-DICHLOROPROPENE</th>\n",
       "      <th>2-CHLOROACETOPHENONE</th>\n",
       "      <th>...</th>\n",
       "      <th>PHOSGENE</th>\n",
       "      <th>PHTHALIC ANHYDRIDE</th>\n",
       "      <th>PROPIONALDEHYDE</th>\n",
       "      <th>PROPYLENE DICHLORIDE (1,2-DICHLOROPROPANE)</th>\n",
       "      <th>PROPYLENE OXIDE</th>\n",
       "      <th>STYRENE OXIDE</th>\n",
       "      <th>TITANIUM TETRACHLORIDE</th>\n",
       "      <th>TRIETHYLAMINE</th>\n",
       "      <th>VINYL ACETATE</th>\n",
       "      <th>1,4-DIOXANE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29168</th>\n",
       "      <td>KY</td>\n",
       "      <td>EPA Region 4</td>\n",
       "      <td>Todd</td>\n",
       "      <td>21219</td>\n",
       "      <td>21219000000</td>\n",
       "      <td>12460</td>\n",
       "      <td>0.394619</td>\n",
       "      <td>5.693090e-10</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.400880e-10</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>7.547389e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3.678371e-06</td>\n",
       "      <td>8.888094e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58853</th>\n",
       "      <td>PA</td>\n",
       "      <td>EPA Region 3</td>\n",
       "      <td>Lackawanna</td>\n",
       "      <td>42069</td>\n",
       "      <td>42069110202</td>\n",
       "      <td>4905</td>\n",
       "      <td>0.265453</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.507246e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.761430e-06</td>\n",
       "      <td>2.002875e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60934</th>\n",
       "      <td>SC</td>\n",
       "      <td>EPA Region 4</td>\n",
       "      <td>Charleston</td>\n",
       "      <td>45019</td>\n",
       "      <td>45019001600</td>\n",
       "      <td>1347</td>\n",
       "      <td>0.590646</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>9.343975e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>4.509885e-07</td>\n",
       "      <td>1.201261e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32483</th>\n",
       "      <td>MA</td>\n",
       "      <td>EPA Region 1</td>\n",
       "      <td>Essex</td>\n",
       "      <td>25009</td>\n",
       "      <td>25009206300</td>\n",
       "      <td>3773</td>\n",
       "      <td>0.337804</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.113101e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>9.372489e-07</td>\n",
       "      <td>1.040940e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12223</th>\n",
       "      <td>CO</td>\n",
       "      <td>EPA Region 8</td>\n",
       "      <td>Denver</td>\n",
       "      <td>8031</td>\n",
       "      <td>8031001302</td>\n",
       "      <td>3926</td>\n",
       "      <td>0.516824</td>\n",
       "      <td>5.738862e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>8.946176e-06</td>\n",
       "      <td>3.308950e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      State    EPA Region      County   FIPS        Tract  Population  \\\n",
       "29168    KY  EPA Region 4        Todd  21219  21219000000       12460   \n",
       "58853    PA  EPA Region 3  Lackawanna  42069  42069110202        4905   \n",
       "60934    SC  EPA Region 4  Charleston  45019  45019001600        1347   \n",
       "32483    MA  EPA Region 1       Essex  25009  25009206300        3773   \n",
       "12223    CO  EPA Region 8      Denver   8031   8031001302        3926   \n",
       "\n",
       "       Total Respiratory (hazard quotient)  1,2-EPOXYBUTANE  \\\n",
       "29168                             0.394619     5.693090e-10   \n",
       "58853                             0.265453     0.000000e+00   \n",
       "60934                             0.590646     0.000000e+00   \n",
       "32483                             0.337804     0.000000e+00   \n",
       "12223                             0.516824     5.738862e-06   \n",
       "\n",
       "       1,3-DICHLOROPROPENE  2-CHLOROACETOPHENONE  ...  PHOSGENE  \\\n",
       "29168             0.000041              0.000004  ...  0.000000   \n",
       "58853             0.000006              0.000000  ...  0.000000   \n",
       "60934             0.000001              0.000008  ...  0.000000   \n",
       "32483             0.000001              0.000000  ...  0.000000   \n",
       "12223             0.000002              0.000018  ...  0.000123   \n",
       "\n",
       "       PHTHALIC ANHYDRIDE  PROPIONALDEHYDE  \\\n",
       "29168        3.400880e-10         0.000405   \n",
       "58853        0.000000e+00         0.000633   \n",
       "60934        0.000000e+00         0.001647   \n",
       "32483        0.000000e+00         0.002920   \n",
       "12223        0.000000e+00         0.004331   \n",
       "\n",
       "       PROPYLENE DICHLORIDE (1,2-DICHLOROPROPANE)  PROPYLENE OXIDE  \\\n",
       "29168                                    0.000006     7.547389e-08   \n",
       "58853                                    0.000008     1.507246e-07   \n",
       "60934                                    0.000006     9.343975e-05   \n",
       "32483                                    0.000004     1.113101e-06   \n",
       "12223                                    0.000004     0.000000e+00   \n",
       "\n",
       "       STYRENE OXIDE  TITANIUM TETRACHLORIDE  TRIETHYLAMINE  VINYL ACETATE  \\\n",
       "29168            0.0                     0.0       0.000014   3.678371e-06   \n",
       "58853            0.0                     0.0       0.000005   1.761430e-06   \n",
       "60934            0.0                     0.0       0.000041   4.509885e-07   \n",
       "32483            0.0                     0.0       0.000014   9.372489e-07   \n",
       "12223            0.0                     0.0       0.000025   8.946176e-06   \n",
       "\n",
       "        1,4-DIOXANE  \n",
       "29168  8.888094e-08  \n",
       "58853  2.002875e-07  \n",
       "60934  1.201261e-05  \n",
       "32483  1.040940e-06  \n",
       "12223  3.308950e-06  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epa_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing \n",
    "\n",
    "We can now start manipulating the original dataset using the predefined functions above.\n",
    "\n",
    "Keep in mind, this dataset has data from all states in the US. In order to use the functions above, we must filter out all the other states and counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the names of the boroughs of NYC (as they appear in the dataset) in an array.\n",
    "FiveBoroughs = [\"Bronx\", \"Kings\", \"New York\", \"Queens\", \"Richmond\"]\n",
    "# Filtering out all the other states\n",
    "epa = epa_df.loc[epa_df.State == 'NY']\n",
    "# Filtering out all the counties except the five boroughs of NYC\n",
    "epa = epa[epa[\"County\"].isin(FiveBoroughs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now determine the geographical units of the original dataset. We could either do this by manually looking at the dataset or using `fips_check` function we defined earlier. The results ensure us we do not have any FIPS code longer than 11 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fips_check(epa, 'Tract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `fips_11()` function and save it in a new dataframe. As described above, this function will ensures we only get the columns of interest in our final dataset. However, in this case we need to remove the previous geogrpahical unit column, `Tract`. `fips_11()` creates a column, _CT_ which serves as the new geographical unit column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epa_11 = fips_11(epa, ['Tract', 'Total Respiratory (hazard quotient)'])\n",
    "epa_11.drop(columns='Tract', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## social Vulnerability Index (SVI)\n",
    "\n",
    "The SVI is developed by CDC (https://svi.cdc.gov/data-and-tools-download.html) to evaluate community risk to hazardous events.\"A number of factors, including poverty, lack of access to transportation, and crowded housing may weaken a community’s ability to prevent human suffering and financial loss in a disaster. These factors are known as social vulnerability\" \n",
    "\n",
    "Centers for Disease Control and Prevention/ Agency for Toxic Substances and Disease Registry/ Geospatial Research, Analysis, and Services Program. Social Vulnerability Index 2016 Database New York. data-and-tools-download.html. Accessed on 2018.\n",
    "\n",
    "### Reading the dataset\n",
    "Similar to EPA's dataset, we read this dataset via direct URL from CDC's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_svi = \"https://svi.cdc.gov/Documents/Data/2016_SVI_Data/CSV/States/NewYork.csv\"\n",
    "svi_df = pd.read_csv(url_svi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset\n",
    "This dataset contains all counties in New York state. We only need the subset of the dataset for the 5 boroughs in NY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FiveBoroughs = ['Bronx', 'Kings', 'New York', 'Queens', 'Richmond']\n",
    "svi = svi_df[svi_df['COUNTY'].isin(FiveBoroughs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step\n",
    "We are now ready to use our predefined functions to trim the columns and the FIPS codes. In this case we are using `fips_11` function which ensures we are extracting census tract level data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_11 = fips_11(svi, ['FIPS', 'RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4', 'RPL_THEMES'])\n",
    "svi_11.drop(columns='FIPS', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food Access Research Atlas from Economic Research Service \n",
    "\n",
    "https://www.ers.usda.gov/data-products/food-access-research-atlas/ (Data is from 2015.  Data version downloaded was last updated 5/18/2017. Data was downloaded 2018).\n",
    "\n",
    "This is a measure of food access based on supermarket accessibility\n",
    "This is a binary variable of YES[1]/NO[0] low access to food.  (If an area has low food access, the value will be 1). \n",
    "\n",
    "### Reading the dataset\n",
    "\n",
    "Similar to other datasets, we will be reading the dataset via URL from USDA's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_food = \"https://www.ers.usda.gov/webdocs/DataFiles/80591/DataDownload2015.xlsx?v=0\"\n",
    "food_df = pd.read_excel(url_food, sheet_name='Food Access Research Atlas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "This dataset also contains data elements for all states in the United States. We only want 5 boroughs of NYC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FiveBoroughs = [\"Bronx\", \"Kings\", \"New York\", \"Queens\", \"Richmond\"]\n",
    "food = food_df.loc[food_df.State == \"New York\"]\n",
    "food = food[food[\"County\"].isin(FiveBoroughs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_11 = fips_11(food, [\"CensusTract\", \"LAhalfand10\"])\n",
    "food_11.drop(columns='CensusTract', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC Open \n",
    "\n",
    "We used _NYC Open Data_ (https://opendata.cityofnewyork.us/), to access sevral datasets for many of the variables used in VACCINe.\n",
    "\n",
    "- Trees\n",
    "- Bus Stop \n",
    "- Subway Stops \n",
    "- Crime Data \n",
    "\n",
    "We downloaded a local copy of each dataset and converted pairs of latitude and logitude to FIPS code using the function defined above. We broke up the larger datasets into smaller subsets before conversion, to prevent potential API timeouts. In this walkthrough we will dedicate a section to using the `geo_to_fips` function. However, for the remainder of the walkthrough we will use the already converted and local version of the datasets.\n",
    "\n",
    "## Trees\n",
    "\n",
    "This measure is a count of alive trees (according to NYC Open dataset) in each geographical area in 2015. \n",
    "\n",
    "https://data.cityofnewyork.us/Environment/2015-Street-Tree-Census-Tree-Data/uvpi-gqnh\n",
    "\n",
    "### Reading the dataset\n",
    "We have uploaded the converted dataset on our GitHub repository, however, you may download the original dataset from the link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_tree = \"https://github.com/SajjadAbedian/SDH_variables/blob/master/treeLocation.zip?raw=true\"\n",
    "r = requests.get(url_tree)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "tree_df = pd.read_csv(z.open('treeLocation.csv')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "This dataset is a subset of the original data. We only selected trees with status of _alive_. At this step we only need to find the count of trees in each geographical unit. We use _Census_Tract_ column to aggregate the rows. We may use _FIPS_ (15-digits code) or _Block_Group_ columns as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = fips_11(tree_df, ['fips', 'tree_id'])\n",
    "tree_11 = pd.value_counts(tree['CT']).to_frame('tree_count')\n",
    "tree_11['CT'] = tree_11.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bus shelters\n",
    "\n",
    "This measure is not a count of subway lines but a binary YES[1]/NO[0] value. (Times square has multiple lines and more connected while some other borough stops will have only one subway line).\n",
    "\n",
    "### Source\n",
    "https://data.cityofnewyork.us/Transportation/Bus-Stop-Shelters/qafz-7myz \n",
    "The raw dataset has latitude and longitude coordinates alongwith a few other data points for each bus shelter in NYC.\n",
    "\n",
    "For our purpose, we need to create a binary indicator for each geographical unit to identify areas with or without bus shelters. Another way of manipulating the raw dataset would be to count the number of bus shelters in each area. (Please see Tree dataset).\n",
    "\n",
    "### Reading Raw Dataset\n",
    "**NOTE** we are also reading a preprocessed file called `bus_df` which has the latitude and longitude pairs converted to FIPS code using `geo_to_fips()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/SajjadAbedian/SDH_variables/master/bustopLocation.csv\"\n",
    "bus_df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Binary Indicators\n",
    "In this step we first count the number of bus shelters in each geographical unit. As mentioned earlier, you could stop and use this data element as one of your variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus = fips_11(bus_df, ['FIPS', 'Location'])\n",
    "bus_11 = pd.value_counts(bus['CT']).to_frame('BusStopCount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to flag the geographical units with at least one bus shelter with `1` and `0` for geographical units with no bus shelter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_11['busStopYN'] = np.where(bus_11['BusStopCount'] > 0, 1, 0)\n",
    "bus_11['CT'] = bus_11.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subway Stations & Subway Entrances \n",
    "\n",
    "In order to accurately identify geographical units with ACCESS to a subway line we need to look at subway entrances as well as subway stations. We need to combine both variables with an OR logical operator. The subway entrance and subway location are in slightly different census tracts or census block groups.  This happens if there are some subways on the border of a geographical area.  \n",
    "\n",
    "For example, central park has 6 subway stations but the surrounding census tracts do not appear to have subways.  The combination of the two data sets aims to better portray access to subway stations. We understand this to be a limitation of this variable, especially at smaller geographical areas.\n",
    "\n",
    "This measure is not a counmt of subway lines but a binary YES[1]/NO[0] value. (Times square has multiple lines and more connected while some other borough stops will have only one subway line). \n",
    "\n",
    "### Data Sources\n",
    "#### Subway Stations\n",
    "https://data.cityofnewyork.us/Transportation/Subway-Stations/arq3-7z49\n",
    "\n",
    "#### Subway Entrance\n",
    "https://data.cityofnewyork.us/Transportation/Subway-Entrances/drex-xx56 \n",
    "\n",
    "### Reading the dataset\n",
    "The raw datasets each have a row per subway line/entrance identified by latitude and longitude. Similar to other datasets, we are using a preprocessed dataset where we already converted the pair of latitudes and longitudes to FIPS codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_loc = \"https://raw.githubusercontent.com/SajjadAbedian/SDH_variables/master/subwayLocation.csv\"\n",
    "url_entrance = \"https://raw.githubusercontent.com/SajjadAbedian/SDH_variables/master/subwayEntrance.csv\"\n",
    "\n",
    "subloc_df = pd.read_csv(url_loc)\n",
    "subentrance_df = pd.read_csv(url_entrance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Binary Indicators\n",
    "In this step we first count the number of subway lines/entrances in each geographical unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subloc = fips_11(subloc_df, ['FIPS', 'NAME'])\n",
    "subloc_11 = pd.value_counts(subloc['CT']).to_frame('SubwayCount')\n",
    "subloc_11['station_yn'] = np.where(subloc_11['SubwayCount'] > 0, 1, 0)\n",
    "subloc_11['CT'] = subloc_11.index\n",
    "\n",
    "subentrance = fips_11(subentrance_df, ['FIPS', 'NAME'])\n",
    "subentrance_11 = pd.value_counts(subentrance['CT']).to_frame('EntranceCount')\n",
    "subentrance_11['entrance_yn'] = np.where(subentrance_11['EntranceCount'] > 0, 1, 0)\n",
    "subentrance_11['CT'] = subentrance_11.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the two variables\n",
    "In this step we need to combine the two variables in one. In other words, we need to create a final binary indicator where we assign `1` when a geographical area has either a subway station or a subway entrance and `0` for geographical area has neither. We then save the results in a new dataframe `subway`. We will complete this task at the end once we join all variables together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime\n",
    "### Reading the original dataset\n",
    "This dataset is particularly both wide and long. It contains all Felonies, Misdemeanors, and Violations. In order to speed up the processing time, we suggest to breakup the dataset based on the crime's types. You can further speed up the process by only selecting a few columns for your dataset (i.e. Latitude, Longitude, and LAW_CAT_CD).]\n",
    "\n",
    "In this walkthrough we briefly go over breaking up the dataset based on the crime type. However, due to the large size of the datasets, we will use a preprocessed dataset which contains converted latitudes and longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN \n",
    "crime = pd.read_csv('NYPD_Complaint_Data_Historic.csv')\n",
    "crime_felony = crime[crime['LAW_CAT_CD'] == 'FELONY']\n",
    "crime_misdemeanor = crime[crime['LAW_CAT_CD'] == 'MISDEMEANOR']\n",
    "crime_violation = crime[crime['LAW_CAT_CD'] == 'VIOLATION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Felony\n",
    "### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_felony = \"https://raw.githubusercontent.com/SajjadAbedian/SDH_variables/master/crimeLocationFelony.csv\"\n",
    "felony_df= pd.read_csv(url_felony)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Similar to the _tree_ dataset we need to get a count of felonies in each geographical unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "felony = fips_11(felony_df, ['FIPS', 'LAW_CAT_CD'])\n",
    "felony_11 = pd.value_counts(felony['CT']).to_frame('felony_count')\n",
    "felony_11['felony_count'].fillna(0, inplace = True)\n",
    "felony_11['CT'] = felony_11.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violation\n",
    "### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_violation = \"https://raw.githubusercontent.com/SajjadAbedian/SDH_variables/master/crimeLocationViolation.csv\"\n",
    "violation_df= pd.read_csv(url_violation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Similar to the _tree_ dataset we need to get a count of violations in each geographical unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation = fips_11(violation_df, ['FIPS', 'LAW_CAT_CD'])\n",
    "violation_11 = pd.value_counts(violation['CT']).to_frame('violation_count')\n",
    "violation_11['violation_count'].fillna(0, inplace = True)\n",
    "violation_11['CT'] = violation_11.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misdemeanor\n",
    "### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_misdemeanor = \"https://raw.githubusercontent.com/SajjadAbedian/SDH_variables/master/crimeLocationMisdemeanor.csv\"\n",
    "misdemeanor_df= pd.read_csv(url_misdemeanor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Similar to the _tree_ dataset we need to get a count of misdemeanors in each geographical unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "misdemeanor = fips_11(misdemeanor_df, ['FIPS', 'LAW_CAT_CD'])\n",
    "misdemeanor_11 = pd.value_counts(misdemeanor['CT']).to_frame('misdemeanor_count')\n",
    "misdemeanor_11['misdemeanor_count'].fillna(0, inplace = True)\n",
    "misdemeanor_11['CT'] = misdemeanor_11.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "We now have processed all datasets from different sources. We can now combine them all in one dataset. We start with the dataset that has **ALL** the FIPS code (in this case ACS, `dfCT`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "VACCINe = dfCT\n",
    "VACCINe = join_to_vaccine(VACCINe, 'FIPS', epa_11, 'CT')\n",
    "VACCINe = join_to_vaccine(VACCINe, 'FIPS', svi_11, 'CT')\n",
    "VACCINe = join_to_vaccine(VACCINe, 'FIPS', food_11, 'CT')\n",
    "VACCINe = join_to_vaccine(VACCINe, 'FIPS', tree_11, 'CT')\n",
    "VACCINe = join_to_vaccine(VACCINe, 'FIPS', bus_11, 'CT')\n",
    "VACCINe = join_to_vaccine(VACCINe, 'FIPS', subloc_11, 'CT')\n",
    "VACCINe = join_to_vaccine(VACCINe, 'FIPS', subentrance_11, 'CT')\n",
    "VACCINe = join_to_vaccine(VACCINe, 'FIPS', felony_11, 'CT')\n",
    "VACCINe = join_to_vaccine(VACCINe, 'FIPS', violation_11, 'CT')\n",
    "VACCINe = join_to_vaccine(VACCINe, 'FIPS', misdemeanor_11, 'CT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating derived variables\n",
    "In this section, we use total population of each FIPS code (extracted from ACS dataset) and crime dataset to find the percentage of crime per 1000 people in each geographical unit. We then add these additional variables to the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "VACCINe['felonyPer1000PeoplePercent'] = (VACCINe.felony_count * 1000) / (dfCT.totalPopulation)\n",
    "VACCINe['violationPer1000PeoplePercent'] = (VACCINe.violation_count * 1000) / (dfCT.totalPopulation)\n",
    "VACCINe['misdemeanorPer1000PeoplePercent'] = (VACCINe.misdemeanor_count * 1000) / (dfCT.totalPopulation)\n",
    "VACCINe['subway_yn'] = np.where(np.logical_or(VACCINe['station_yn'] == 1, VACCINe['entrance_yn'] == 1), 1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleanup\n",
    "Before exporting the dataset, we need to make sure we do a series of data cleanups. It includes removing values that are out of range. For example, replacing _-666666666_ from ACS dataset and _-999_ from SVI dataset with _-666_. This will help to unify all numbers that are flagged as out of range. We will also fill all NULL values with _-666_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove values that are -666 from census tract data (low or zero populations)\n",
    "VACCINe['medianHouseIncome'] = np.where(VACCINe['medianHouseIncome']== -666666666.0, -666, VACCINe['medianHouseIncome'])\n",
    "VACCINe['GINI'] = np.where(VACCINe['GINI']== -666666666.0, -666, VACCINe['GINI'])\n",
    "# remove -999 values from SVI (low or zero populations)\n",
    "VACCINe['RPL_THEME1'] = np.where(VACCINe['RPL_THEME1']== -999.0000, -666, VACCINe['RPL_THEME1'])\n",
    "VACCINe['RPL_THEME2'] = np.where(VACCINe['RPL_THEME2']== -999.0000, -666, VACCINe['RPL_THEME2'])\n",
    "VACCINe['RPL_THEME3'] = np.where(VACCINe['RPL_THEME3']== -999.0000, -666, VACCINe['RPL_THEME3'])\n",
    "VACCINe['RPL_THEME4'] = np.where(VACCINe['RPL_THEME4']== -999.0000, -666, VACCINe['RPL_THEME4'])\n",
    "VACCINe['RPL_THEMES'] = np.where(VACCINe['RPL_THEMES']== -999.0000, -666, VACCINe['RPL_THEMES'])\n",
    "# remove NaN values (from dividing by a population of zero )\n",
    "VACCINe = VACCINe.fillna(value = -666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the final dataset\n",
    "We can now confidently export the dataset into a .CSV file. It is good practice to inspect the dataset before exporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FIPS',\n",
       " 'foriegnBornPercent',\n",
       " 'woInsurance18_64Percent',\n",
       " 'medicaid18_64Percent',\n",
       " 'medicaid_Medicare18_64Percent',\n",
       " 'no_school_to_12_grade_no_diplomaPercent',\n",
       " 'highSchool_GEDPercent',\n",
       " 'unemployedPercent',\n",
       " 'medianHouseIncome',\n",
       " 'belowPovertyPercent',\n",
       " 'GINI',\n",
       " 'totalPopulation',\n",
       " 'Total Respiratory (hazard quotient)',\n",
       " 'RPL_THEME1',\n",
       " 'RPL_THEME2',\n",
       " 'RPL_THEME3',\n",
       " 'RPL_THEME4',\n",
       " 'RPL_THEMES',\n",
       " 'LAhalfand10',\n",
       " 'tree_count',\n",
       " 'BusStopCount',\n",
       " 'busStopYN',\n",
       " 'SubwayCount',\n",
       " 'station_yn',\n",
       " 'EntranceCount',\n",
       " 'entrance_yn',\n",
       " 'felony_count',\n",
       " 'violation_count',\n",
       " 'misdemeanor_count',\n",
       " 'felonyPer1000PeoplePercent',\n",
       " 'violationPer1000PeoplePercent',\n",
       " 'misdemeanorPer1000PeoplePercent',\n",
       " 'subway_yn']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(VACCINe.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "VACCINe.to_csv('VACCINe_' + str(fips_check(VACCINe, 'FIPS')) + '_digits_fips_'+ str(date.today()) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
